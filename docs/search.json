[
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Our Course Project",
    "section": "",
    "text": "I’m honored to be a member of the [group name] project team.\nBelow, you’ll find a brief summary of our project. To access a detailed project description, please go to https://[your-project-url].\nSummary\n[provide a biref summary of your project]\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to My Analytics Lab",
    "section": "",
    "text": "Hello! My name is Şadiye Öztürk.\nAlso my first webpage design experiment.\nPlease stay tuned to follow my works on data analytics, blog posts, and more.\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "assignments/assignment-2.html",
    "href": "assignments/assignment-2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "If you are not Erdi Dasdemir and you are here, you are probably looking for information about the assignmenr. I would be happy if you could send me your suggestions for my homework. If you need help, you can reach me via Slack. I would be happy if you use my assignment as a helpful reference."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Download CV (English)\n\nEducation\nB.S., Industrial Engineering, Hacettepe University, Turkey, 2019 - ongoing.\n\n\nWork Experience\n\nTürk Telekomünikasyon A.Ş., IT Business Analyst, 08/01/2023-ongoing.\n\n\n\nInternships\n\nGEMAK Food Industrial Machinery and Inc., Summer Intern, 2022\nTurkish Aerospace Industries Inc., Summer Intern, 2023\n\n\n\nProjects\n\nNÜVE Industrial Materials Manufacturing and Trade Inc., Graduation Project, 2023- ongoing\n\n\n\nSkills\n\n\n\nTools & Technologies\nCompetency Level\nCertification\n\n\n\n\nSQL\nAdvanced\nUdemy\n\n\nExcel\nAdvanced\nUdemy\n\n\nExpertfit\nIntermediate\n\n\n\nMinitab\nIntermediate\n\n\n\nPython\nIntermediate\n\n\n\nSolidworks\nIntermediate\n\n\n\nR\nBeginner\nCoursera\n\n\nMatlab\nIntermediate\n\n\n\nMS Office\nAdvanced\n\n\n\n\n\n\nSeminars & Certification\n\nGoogle Data Analytics Professional Certificate, 2023\nGoogle Technical Support Fundamentals, 2023\nSabancı Womentum Education Program, 2023\n\n\n\nHobbies\n\nHiking\nYoga\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "about.html#employements",
    "href": "about.html#employements",
    "title": "About Me",
    "section": "Employements",
    "text": "Employements\n\nFirm xxx, position xx, year xxx\nFirm yyy, position yyy, year yyy"
  },
  {
    "objectID": "about.html#internships",
    "href": "about.html#internships",
    "title": "About Me",
    "section": "Internships",
    "text": "Internships\n\nFirm aaa, position xx, year xxx\nFirm bbb, position yyy, year yyy"
  },
  {
    "objectID": "assignments/assignment-1.html",
    "href": "assignments/assignment-1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "My first assignment has three parts."
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "My Assignments",
    "section": "",
    "text": "On this page, I showcase the assignment I conducted for the Fall 2023 EMU 430 Data Analytics course.\nPlease use left menu to navigate through my assignments.\nThe most recent update to this page was made on November 3, 2023\n\n\n\n Back to top"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Data Camp Notes",
    "section": "",
    "text": "CLEANING DATA IN R\n\nlibrary(dslabs)\ndata(murders)\nhead(murders) #gives the first 6 row from data set\n\n       state abb region population total\n1    Alabama  AL  South    4779736   135\n2     Alaska  AK   West     710231    19\n3    Arizona  AZ   West    6392017   232\n4   Arkansas  AR  South    2915918    93\n5 California  CA   West   37253956  1257\n6   Colorado  CO   West    5029196    65\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nglimpse(murders) #glimpsing of the data set\n\nRows: 51\nColumns: 5\n$ state      &lt;chr&gt; \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"…\n$ abb        &lt;chr&gt; \"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"DC\", \"FL\",…\n$ region     &lt;fct&gt; South, West, West, South, West, West, Northeast, South, Sou…\n$ population &lt;dbl&gt; 4779736, 710231, 6392017, 2915918, 37253956, 5029196, 35740…\n$ total      &lt;dbl&gt; 135, 19, 232, 93, 1257, 65, 97, 38, 99, 669, 376, 7, 12, 36…\n\nstr(murders) #structure of the data set\n\n'data.frame':   51 obs. of  5 variables:\n $ state     : chr  \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" ...\n $ abb       : chr  \"AL\" \"AK\" \"AZ\" \"AR\" ...\n $ region    : Factor w/ 4 levels \"Northeast\",\"South\",..: 2 4 4 2 4 4 1 2 2 2 ...\n $ population: num  4779736 710231 6392017 2915918 37253956 ...\n $ total     : num  135 19 232 93 1257 ...\n\nis.numeric(murders$population)\n\n[1] TRUE\n\n#if we use assert_is_numeric(murders$state) the output would be \"murders$state is not class 'numeric'; it has class chr. (You should install required packages for use assert_is_ functions.)\n\nlibrary(stringr)\nrevenue_trimmed = str_remove(murders$abb, \"A\") #if we would like to remove a chr from a string we can use str_remove func.\nrevenue_trimmed\n\n [1] \"L\"  \"K\"  \"Z\"  \"R\"  \"C\"  \"CO\" \"CT\" \"DE\" \"DC\" \"FL\" \"G\"  \"HI\" \"ID\" \"IL\" \"IN\"\n[16] \"I\"  \"KS\" \"KY\" \"L\"  \"ME\" \"MD\" \"M\"  \"MI\" \"MN\" \"MS\" \"MO\" \"MT\" \"NE\" \"NV\" \"NH\"\n[31] \"NJ\" \"NM\" \"NY\" \"NC\" \"ND\" \"OH\" \"OK\" \"OR\" \"P\"  \"RI\" \"SC\" \"SD\" \"TN\" \"TX\" \"UT\"\n[46] \"VT\" \"V\"  \"W\"  \"WV\" \"WI\" \"WY\"\n\nhead(murders %&gt;% mutate(new_column = revenue_trimmed)) #add a new column to a data frame\n\n       state abb region population total new_column\n1    Alabama  AL  South    4779736   135          L\n2     Alaska  AK   West     710231    19          K\n3    Arizona  AZ   West    6392017   232          Z\n4   Arkansas  AR  South    2915918    93          R\n5 California  CA   West   37253956  1257          C\n6   Colorado  CO   West    5029196    65         CO\n\nsummary(murders$population) #calculate min, 1st Qu, median, mean, 3rd Qu and max values for the variable.\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n  563626  1696962  4339367  6075769  6636084 37253956 \n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "assignments/assignment-1.html#a",
    "href": "assignments/assignment-1.html#a",
    "title": "Assignment 1",
    "section": "(a)",
    "text": "(a)\nWhat is data wrangling? Intro, Motivation, Outline, Setup – Pt. 1 Data Wrangling Introduction\n\nData scientists spend a significant portion of their time collecting and preparing data before analysis.\nR packages like tidyr and dplyr make data work more efficient.\nTable structure allows you to work with large datasets more efficiently, only showing a portion of the data that fits in your console window.\nThe “pipe operator” (%&gt;%) is a handy way to connect data work steps.\nYou can learn to do things like selecting specific data, filtering, creating new info, and summarizing data using these tools."
  },
  {
    "objectID": "assignments/assignment-1.html#b",
    "href": "assignments/assignment-1.html#b",
    "title": "Assignment 1",
    "section": "(b)",
    "text": "(b)\n1.Indexing\n\nPython\nIndexing in Python starts from 0.\n\nx=list(range(1,4))\nfirst_element=x[1]\n#output is 2 because first index is 0 we should use x[0] for first element of the list in python.\nprint(first_element)\n\n2\n\n\nR\nIndexing in R starts from 1.\n\nx &lt;- c(1:4)\nfirst_element &lt;- x[1]\n#when we try it in python output is going to be 2 because indexing starts from 0.\nfirst_element \n\n[1] 1\n\n\n\n2.Style\n\nR\nR use more simple programming language rather then python.\n\ny&lt;-c(1,2,3,4,5)\nsquare&lt;-y^2 \n#using square root of the defined element give us the square of elements on the list.\nsquare\n\n[1]  1  4  9 16 25\n\n\nPython\nWe have to use for loop for calculate the square of the elements on the list.\n\ny = [1,2,3,4,5]\nsq = [y**2 for y in y] \nprint(sq)\n\n[1, 4, 9, 16, 25]\n\n\n\n3.Syntax for Conditional Statements\n\nR\nIfelse applies the condition to each element on the list without for loop.\n\nt &lt;- c(1,2,3,4,5,6)\nifelse(t&lt;4,t+1,0)\n\n[1] 2 3 4 0 0 0\n\n\nPython\nIn python, we need for loop to apply the condition to each element on the list.\n\nt = [1,2,3,4,5,6]\nnew_list = [x + 1 if x &lt; 4 else 0 for x in t]\nprint(new_list)\n\n[2, 3, 4, 0, 0, 0]"
  },
  {
    "objectID": "assignments/assignment-1.html#c",
    "href": "assignments/assignment-1.html#c",
    "title": "Assignment 1",
    "section": "(c)",
    "text": "(c)\n\n#install.packages(\"dslabs\") \n# if you install the packages once there is no need to instaal each time just use for first time is enough.\n\nlibrary(dslabs)\n\ndata(\"na_example\")\n\nprint(na_example) #print na_example\n\n   [1]  2  1  3  2  1  3  1  4  3  2  2 NA  2  2  1  4 NA  1  1  2  1  2  2  1\n  [25]  2  5 NA  2  2  3  1  2  4  1  1  1  4  5  2  3  4  1  2  4  1  1  2  1\n  [49]  5 NA NA NA  1  1  5  1  3  1 NA  4  4  7  3  2 NA NA  1 NA  4  1  2  2\n  [73]  3  2  1  2  2  4  3  4  2  3  1  3  2  1  1  1  3  1 NA  3  1  2  2  1\n  [97]  2  2  1  1  4  1  1  2  3  3  2  2  3  3  3  4  1  1  1  2 NA  4  3  4\n [121]  3  1  2  1 NA NA NA NA  1  5  1  2  1  3  5  3  2  2 NA NA NA NA  3  5\n [145]  3  1  1  4  2  4  3  3 NA  2  3  2  6 NA  1  1  2  2  1  3  1  1  5 NA\n [169] NA  2  4 NA  2  5  1  4  3  3 NA  4  3  1  4  1  1  3  1  1 NA NA  3  5\n [193]  2  2  2  3  1  2  2  3  2  1 NA  2 NA  1 NA NA  2  1  1 NA  3 NA  1  2\n [217]  2  1  3  2  2  1  1  2  3  1  1  1  4  3  4  2  2  1  4  1 NA  5  1  4\n [241] NA  3 NA NA  1  1  5  2  3  3  2  4 NA  3  2  5 NA  2  3  4  6  2  2  2\n [265] NA  2 NA  2 NA  3  3  2  2  4  3  1  4  2 NA  2  4 NA  6  2  3  1 NA  2\n [289]  2 NA  1  1  3  2  3  3  1 NA  1  4  2  1  1  3  2  1  2  3  1 NA  2  3\n [313]  3  2  1  2  3  5  5  1  2  3  3  1 NA NA  1  2  4 NA  2  1  1  1  3  2\n [337]  1  1  3  4 NA  1  2  1  1  3  3 NA  1  1  3  5  3  2  3  4  1  4  3  1\n [361] NA  2  1  2  2  1  2  2  6  1  2  4  5 NA  3  4  2  1  1  4  2  1  1  1\n [385]  1  2  1  4  4  1  3 NA  3  3 NA  2 NA  1  2  1  1  4  2  1  4  4 NA  1\n [409]  2 NA  3  2  2  2  1  4  3  6  1  2  3  1  3  2  2  2  1  1  3  2  1  1\n [433]  1  3  2  2 NA  4  4  4  1  1 NA  4  3 NA  1  3  1  3  2  4  2  2  2  3\n [457]  2  1  4  3 NA  1  4  3  1  3  2 NA  3 NA  1  3  1  4  1  1  1  2  4  3\n [481]  1  2  2  2  3  2  3  1  1 NA  3  2  1  1  2 NA  2  2  2  3  3  1  1  2\n [505] NA  1  2  1  1  3  3  1  3  1  1  1  1  1  2  5  1  1  2  2  1  1 NA  1\n [529]  4  1  2  4  1  3  2 NA  1  1 NA  2  1  1  4  2  3  3  1  5  3  1  1  2\n [553] NA  1  1  3  1  3  2  4 NA  2  3  2  1  2  1  1  1  2  2  3  1  5  2 NA\n [577]  2 NA  3  2  2  2  1  5  3  2  3  1 NA  3  1  2  2  2  1  2  2  4 NA  6\n [601]  1  2 NA  1  1  2  2  3 NA  3  2  3  3  4  2 NA  2 NA  4 NA  1  1  2  2\n [625]  3  1  1  1  3 NA  2  5 NA  7  1 NA  4  3  3  1 NA  1  1  1  1  3  2  4\n [649]  2  2  3 NA NA  1  4  3  2  2  2  3  2  4  2  2  4 NA NA NA  6  3  3  1\n [673]  4  4  2  1 NA  1  6 NA  3  3  2  1  1  6 NA  1  5  1 NA  2  6  2 NA  4\n [697]  1  3  1  2 NA  1  1  3  1  2  4  2  1  3  2  4  3  2  2  1  1  5  6  4\n [721]  2  2  2  2  4 NA  1  2  2  2  2  4  5 NA NA NA  4  3  3  3  2  4  2  4\n [745] NA NA NA NA  2  1 NA  2  4  3  2 NA  2  3  1  3  4 NA  1  2  1  2 NA  3\n [769]  1  2  1  2  1  2  1  2  2  2  2  1  1  3  3  1  3  4  3 NA NA  4  2  3\n [793]  2  1  3  2  4  2  2  3  1  2  4  3  3  4 NA  1  4  2  1  1  1  3  1  5\n [817]  2  2  4  2 NA  1  3  1  2 NA  1  2  1  2  1 NA  1  3  2  3  2 NA  2  1\n [841]  4  2 NA NA NA  2  4  2 NA NA  3  1 NA  5  5  2  2  2 NA  2  1  3  1  3\n [865]  2  4  2  4 NA  4  1  2  3  2  3  3  2  3  2  2  2  1  3  2  4  2 NA  3\n [889]  3  2  2 NA NA  3  2  1  2  4  1  1  1  1  4  3  2 NA  3  2 NA  1 NA  3\n [913]  2  1  1  1  2 NA  2  2  3  3  2 NA NA  4  5  2  2  2  1  2  3  1  3  3\n [937]  4  3 NA  1  1  1 NA  4  3  5  1  1  2 NA  2  2  2  2  5  2  2  3  1  2\n [961]  3 NA  1  2 NA NA  2 NA  3  1  1  2  5  3  5  1  1  4 NA  2  1  3  1  1\n [985]  2  4  3  3  3 NA  1  1  2  2  1  1  2  2 NA  2\n\nna_check &lt;-ifelse(is.na(na_example),1,0) #for sumation check NA and print as 1\n\nsum_na &lt;- sum(na_check) \nsum_na # total numbers of NA\n\n[1] 145\n\nwithout_na&lt;-ifelse(is.na(na_example),0,na_example) #turn the na values to 0\n\nupdated_num_na&lt;-sum(ifelse(is.na(without_na),1,0))\nupdated_num_na\n\n[1] 0"
  },
  {
    "objectID": "assignments/assignment-2.html#importing-data-to-rstudio",
    "href": "assignments/assignment-2.html#importing-data-to-rstudio",
    "title": "Assignment 2",
    "section": "1. Importing Data to RStudio",
    "text": "1. Importing Data to RStudio\n\nsuppressMessages(library(tidyverse)) # for everything :)\nsuppressMessages(library(rvest)) # for HTML scraping\nsuppressMessages(library(stringr)) # for string processing\n\nurl_1 &lt;- \"https://m.imdb.com/search/title/?title_type=feature&release_date=2010-01-01,2024-01-01&sort=release_date,desc&num_votes=2498,&country_of_origin=TR&count=250\"\nurl_2 &lt;- \"https://m.imdb.com/search/title/?title_type=feature&release_date=,2010-01-01&sort=release_date,desc&num_votes=2498,&country_of_origin=TR&count=250\"\n\ndata_html_1 &lt;- read_html(url_1)\ndata_html_2 &lt;- read_html(url_2)\n\nFirstly,I called the libraries I needed by typing them at the beginning of my code. Then, I import data to R. Since I could not pull all the data at once, I divided the data into two according to dates."
  },
  {
    "objectID": "assignments/assignment-2.html#start-web-scrapping-to-create-a-data-frame-with-columns-title-year-durationrating-votes",
    "href": "assignments/assignment-2.html#start-web-scrapping-to-create-a-data-frame-with-columns-title-year-durationrating-votes",
    "title": "Assignment 2",
    "section": "2. Start web scrapping to create a Data Frame with columns: Title, Year, Duration,Rating, Votes",
    "text": "2. Start web scrapping to create a Data Frame with columns: Title, Year, Duration,Rating, Votes\n\ntitle_names_1 &lt;-html_nodes(data_html_1,'.ipc-title__text')\ntitle_names_1 &lt;- html_text(title_names_1)\ntitle_names_1 &lt;- tail(head(title_names_1,-1),-1)\ntitle_names_1 &lt;- str_split(title_names_1, \" \", n=2)\ntitle_names_1 &lt;- unlist(lapply(title_names_1, function(x) {x[2]}))\n\ntitle_names_2 &lt;-html_nodes(data_html_2,'.ipc-title__text')\ntitle_names_2 &lt;- html_text(title_names_2)\ntitle_names_2 &lt;- tail(head(title_names_2,-1),-1)\ntitle_names_2 &lt;- str_split(title_names_2, \" \", n=2)\ntitle_names_2 &lt;- unlist(lapply(title_names_2, function(x) {x[2]}))\n\ntitles&lt;-c(title_names_1,title_names_2)\n\n\nyear_1 &lt;- html_elements(data_html_1,'.dli-title-metadata &gt; span:nth-child(1)')\nyear_1 &lt;- html_text(year_1)\n\nyear_2 &lt;- html_elements(data_html_2,'.dli-title-metadata &gt; span:nth-child(1)')\nyear_2 &lt;- html_text(year_2)\n\nyear &lt;- as.numeric(c(year_1,year_2))\n\n\nduration_1 &lt;- html_elements(data_html_1,'.dli-title-metadata &gt; span:nth-child(2)')\nduration_1 &lt;- html_text(duration_1)\n\nduration_2 &lt;- html_elements(data_html_2,'.dli-title-metadata &gt; span:nth-child(2)')\nduration_2 &lt;- html_text(duration_2)\n\ndurations &lt;- c(duration_1,duration_2)\n\nrating_1 &lt;- html_elements(data_html_1,'.ipc-rating-star--imdb')\nrating_1 &lt;- html_text(rating_1)\ncleaned_rating_1 &lt;- gsub(\"[^0-9]\", \"\", rating_1)\nfirst_two_digits_1 &lt;- substr(cleaned_rating_1, 1, 2)\nnumeric_rating_1 &lt;-as.numeric(first_two_digits_1)/10\n\nrating_2 &lt;- html_elements(data_html_2,'.ipc-rating-star--imdb')\nrating_2 &lt;- html_text(rating_2)\ncleaned_rating_2 &lt;- gsub(\"[^0-9]\", \"\", rating_2)\nfirst_two_digits_2 &lt;- substr(cleaned_rating_2, 1, 2)\nnumeric_rating_2 &lt;-as.numeric(first_two_digits_2)/10\n\n\nrating &lt;- c(numeric_rating_1, numeric_rating_2)\n\nvote_1 &lt;- html_elements(data_html_1,'.kRnqtn')\nvote_1 &lt;- html_text(vote_1)\ncleaned_vote_1 &lt;- gsub(\"[^0-9]\", \"\", vote_1)\nnumeric_vote_1 &lt;- as.numeric(gsub(\",\", \"\", cleaned_vote_1))\n\n\nvote_2 &lt;- html_elements(data_html_2,'.kRnqtn')\nvote_2 &lt;- html_text(vote_2)\ncleaned_vote_2 &lt;- gsub(\"[^0-9]\", \"\", vote_2)\nnumeric_vote_2 &lt;- as.numeric(gsub(\",\", \"\", cleaned_vote_2))\n\nvotes &lt;- c(numeric_vote_1,numeric_vote_2)\n\nsuppressWarnings(hours &lt;- ifelse(is.na(as.numeric(sub(\"h.*\", \"\",durations))),0,as.numeric(sub(\"h.*\", \"\",durations))))\n\nminutes &lt;- sub(\".*\\\\s(\\\\d+)m\", \"\\\\1\", durations)\nminutes&lt;-ifelse(grepl(\"h\", minutes),0,minutes) \nminutes&lt;-ifelse(grepl(\"m\",minutes),gsub(\"m\", \"\", minutes),minutes)\nminutes &lt;- as.numeric(minutes)\n\nduration_min &lt;-60*hours+minutes\n\nimdb_data_frame &lt;- data.frame(Title=titles,Year=year,Duration=durations, Rating=rating, Votes=votes, DurationMin = duration_min)\n\nTo create a data frame, I separated and combined the headers from each of the URLs I took separately and finally created a data frame by combining all the headers."
  },
  {
    "objectID": "assignments/assignment-2.html#exploratory-data-analysis",
    "href": "assignments/assignment-2.html#exploratory-data-analysis",
    "title": "Assignment 2",
    "section": "3. Exploratory Data Analysis",
    "text": "3. Exploratory Data Analysis\n\na) The 5 best and worst movies according to rating\n\nindex &lt;- order(imdb_data_frame$Rating,decreasing = TRUE )\ntop_5 &lt;- head(index,5)\nimdb_data_frame[top_5[1:5],]\n\n                           Title Year Duration Rating Votes DurationMin\n457               Hababam Sinifi 1975   1h 27m    9.2 42513          87\n202       CM101MMXI Fundamentals 2013   2h 19m    9.1 46995         139\n446                   Tosun Pasa 1976   1h 30m    8.9 24327          90\n453 Hababam Sinifi Sinifta Kaldi 1975   1h 35m    8.9 24370          95\n452                Süt Kardesler 1976   1h 20m    8.8 20886          80\n\nindex_1 &lt;- order(imdb_data_frame$Rating)\nbottom_5 &lt;- head(index_1,5)\nimdb_data_frame[bottom_5[1:5],]   \n\n                             Title Year Duration Rating Votes DurationMin\n100 Cumali Ceber: Allah Seni Alsin 2017   1h 40m    1.0 39267         100\n110                           Reis 2017   1h 48m    1.0 73973         108\n21                           Müjde 2022      48m    1.2  9920          48\n28               15/07 Safak Vakti 2021   1h 35m    1.2 20607          95\n75                  Cumali Ceber 2 2018   1h 40m    1.2 10228         100\n\n\nFor the last 5, I would like it to be in the Recep Ivedik series, even though the whole of Turkey does not agree with me, but this last five is quite bad and appropriate. For the top 5, Cem Yılmaz is not a comedian I like very much, but CM101 was good, other than that, it’s a nostalgic 5. I think the average age of those who voted is high, and I also think that the actors got such high ratings because of their respectability. Both lists seem appropriate for me.\n\n\nb) My favorite movies\n\nmy_fav_movies &lt;- imdb_data_frame[grepl(\"Dedemin Insanlari\", ignore.case = TRUE, imdb_data_frame$Title) |\n                                   grepl(\"Ayla\", ignore.case = TRUE, imdb_data_frame$Title), ]\n\nmy_fav_movies\n\n                        Title Year Duration Rating Votes DurationMin\n99  Ayla: The Daughter of War 2017    2h 5m    8.3 42990         125\n218         Dedemin Insanlari 2011    2h 6m    8.0 11216         126\n\n\n“Dedemin İnsanları” is a very special movie for me and I think it did not get the rating it deserved. Since I cannot be objective on this issue, I may interpret it this way. Ayla is one of the best Turkish movies I have watched and its rating is neither more nor less.\n\n\nc) Average ratings of Turkish movies\n\nyearly_avg_ratings &lt;- imdb_data_frame %&gt;%\n  group_by(Year) %&gt;%\n  summarize(avg_rating = mean(Rating))\n\nggplot(yearly_avg_ratings, aes(x = Year, y = avg_rating)) +\n  geom_point() +\n  labs(title = \"Yearly Average Ratings of Turkish Movies\", x = \"Year\", y = \"Average Rating\") +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))  \n\n\n\nmovie_counts &lt;- imdb_data_frame %&gt;%\n  group_by(Year) %&gt;%\n  summarise(count = n())\n\n\nggplot(imdb_data_frame, aes(x =factor(Year) , y = Rating)) +\n  geom_boxplot(fill = \"skyblue\") +\n  labs(title = \"Distribution of Ratings Over the Years\", x = \"Year\", y = \"Rating\")+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))  \n\n\n\n\nAlthough movies have increased over the years, their ratings have gradually decreased. I attribute this to the fact that all YouTubers in recent years have been making movies and movies that have no artistic quality and are made only for money purposes.\n\n\nd) Correlation Between Votes and Ratings\n\ncorrelation_1 &lt;- cor(imdb_data_frame$Votes, imdb_data_frame$Rating)\ncorrelation_1\n\n[1] 0.1307548\n\n\nAs can be seen, the correlation is very close to 0. There is no linear relationship between two variables.\n\n\ne) Correlation Between Duration and Ratings\n\ncorrelation_2 &lt;- cor(imdb_data_frame$DurationMin, imdb_data_frame$Rating)\ncorrelation_2\n\n[1] 0.03343216\n\n\nAs can be seen, this means that the correlation is closer to 0 than the previous result. There is no linear relationship between two variables."
  },
  {
    "objectID": "assignments/assignment-2.html#imdb-top-1000",
    "href": "assignments/assignment-2.html#imdb-top-1000",
    "title": "Assignment 2",
    "section": "4. IMDb Top 1000",
    "text": "4. IMDb Top 1000\n\nurl_3 &lt;- \"https://m.imdb.com/search/title/?title_type=feature&groups=top_1000&country_of_origin=TR\"\n\ndata_html_3 &lt;- read_html(url_3)\n\ntitle_names_3 &lt;-html_nodes(data_html_3,'.ipc-title__text')\ntitle_names_3 &lt;- html_text(title_names_3)\ntitle_names_3 &lt;- tail(head(title_names_3,-1),-1)\ntitle_names_3 &lt;- str_split(title_names_3, \" \", n=2)\ntitle_names_3 &lt;- unlist(lapply(title_names_3, function(x) {x[2]}))\n\nyear_3 &lt;- html_elements(data_html_3,'.dli-title-metadata &gt; span:nth-child(1)')\nyear_3 &lt;- html_text(year_3)\nyear_3 &lt;- as.factor(year_3)\n\n\nimdb_data_frame_top1000 &lt;- data.frame(Title=title_names_3,Year=year_3)\n\nimdb_data_frame_top1000\n\n                       Title Year\n1   Yedinci Kogustaki Mucize 2019\n2                 Kis Uykusu 2014\n3      Nefes: Vatan Sagolsun 2009\n4  Ayla: The Daughter of War 2017\n5             Babam ve Oglum 2005\n6                Ahlat Agaci 2018\n7    Bir Zamanlar Anadolu'da 2011\n8                     Eskiya 1996\n9                   G.O.R.A. 2004\n10                 Vizontele 2001\n11  Her Sey Çok Güzel Olacak 1998\n\n\n\nJoin the data frames\n\n\nimdb_data_frame_top1000$Year &lt;- as.numeric(as.character(imdb_data_frame_top1000$Year)) #Since my code gave an error, I asked chatgpt about the error and she offered such a solution.\n\nmerged_data &lt;- left_join(imdb_data_frame_top1000,imdb_data_frame, by = c(\"Title\", \"Year\"))\nmerged_data\n\n                       Title Year Duration Rating Votes DurationMin\n1   Yedinci Kogustaki Mucize 2019   2h 12m    8.2 54156         132\n2                 Kis Uykusu 2014   3h 16m    8.0 54633         196\n3      Nefes: Vatan Sagolsun 2009    2h 8m    8.0 35019         128\n4  Ayla: The Daughter of War 2017    2h 5m    8.3 42990         125\n5             Babam ve Oglum 2005   1h 48m    8.2 91026         108\n6                Ahlat Agaci 2018    3h 8m    8.0 27003         188\n7    Bir Zamanlar Anadolu'da 2011   2h 37m    7.8 49354         157\n8                     Eskiya 1996    2h 8m    8.1 71699         128\n9                   G.O.R.A. 2004    2h 7m    8.0 66029         127\n10                 Vizontele 2001   1h 50m    8.0 38400         110\n11  Her Sey Çok Güzel Olacak 1998   1h 47m    8.1 27119         107\n\n\n\nThe top 11 movies from first data frame based on their rank\n\n\nindex_2 &lt;- order(imdb_data_frame$Rating,decreasing = TRUE )\ntop_11 &lt;- head(index_2,11)\ntop_11 &lt;- imdb_data_frame[top_11[1:11],]\ntop_11\n\n                           Title Year Duration Rating Votes DurationMin\n457               Hababam Sinifi 1975   1h 27m    9.2 42513          87\n202       CM101MMXI Fundamentals 2013   2h 19m    9.1 46995         139\n446                   Tosun Pasa 1976   1h 30m    8.9 24327          90\n453 Hababam Sinifi Sinifta Kaldi 1975   1h 35m    8.9 24370          95\n452                Süt Kardesler 1976   1h 20m    8.8 20886          80\n389                   Zügürt Aga 1985   1h 41m    8.7 16134         101\n435                  Kibar Feyzo 1978   1h 23m    8.7 17126          83\n438                Neseli Günler 1978   1h 35m    8.7 11806          95\n442             Saban Oglu Saban 1977   1h 30m    8.7 18535          90\n449      Hababam Sinifi Uyaniyor 1976   1h 34m    8.7 20640          94\n445       Hababam Sinifi Tatilde 1977   1h 37m    8.6 18637          97\n\n\n\nIt’s really strange that there isn’t even a single movie matching the IMDb top 1000 in the first data frame. I did research on this subject and IMDb does not provide an explanation as to how it determines the top 1000. I cannot say that I achieved much by analyzing the data. I think this is related to the up-to-dateness of the data, although I cannot base it on any concrete evidence."
  },
  {
    "objectID": "assignments/assignment-2.html#hello",
    "href": "assignments/assignment-2.html#hello",
    "title": "Assignment 2",
    "section": "",
    "text": "If you are not Erdi Dasdemir and you are here, you are probably looking for information about the assignmenr. I would be happy if you could send me your suggestions for my homework. If you need help, you can reach me via Slack. I would be happy if you use my assignment as a helpful reference."
  }
]